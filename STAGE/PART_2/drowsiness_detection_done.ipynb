{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "\n",
    "# Verify that TensorFlow is using the GPU\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_landmarks(frame, face_detector, landmark_predictor):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector(gray_frame)\n",
    "    \n",
    "    landmarks_list = []\n",
    "    for face in faces:\n",
    "        landmarks = landmark_predictor(gray_frame, face)\n",
    "        landmarks_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        landmarks_list.append(landmarks_points)\n",
    "    \n",
    "    return landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "def play_alert_sound():\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(\"../alert.mp3\")  # Replace \"alert_sound.wav\" with the path to your sound file.\n",
    "    pygame.mixer.music.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eyes_from_landmarks(frame, landmarks):\n",
    "    \n",
    "    if len(landmarks) != 68:\n",
    "        raise ValueError(\"Facial landmarks should contain 68 points.\")\n",
    "\n",
    "    # Define the indices for the left and right eyes based on facial landmarks.\n",
    "    left_eye_indices = [i for i in range(36, 42)]\n",
    "    right_eye_indices = [i for i in range(42, 48)]\n",
    "\n",
    "    # Extract left and right eye regions from the frame.\n",
    "    left_eye_coords = np.array([landmarks[i] for i in left_eye_indices], dtype=np.int32)\n",
    "    right_eye_coords = np.array([landmarks[i] for i in right_eye_indices], dtype=np.int32)\n",
    "\n",
    "    # Calculate the bounding boxes for the left and right eye regions.\n",
    "    left_eye_x = np.min(left_eye_coords[:, 0])\n",
    "    left_eye_y = np.min(left_eye_coords[:, 1])\n",
    "    left_eye_w = np.max(left_eye_coords[:, 0]) - left_eye_x\n",
    "    left_eye_h = np.max(left_eye_coords[:, 1]) - left_eye_y\n",
    "\n",
    "    right_eye_x = np.min(right_eye_coords[:, 0])\n",
    "    right_eye_y = np.min(right_eye_coords[:, 1])\n",
    "    right_eye_w = np.max(right_eye_coords[:, 0]) - right_eye_x\n",
    "    right_eye_h = np.max(right_eye_coords[:, 1]) - right_eye_y\n",
    "\n",
    "    # Extend the cropping region to include the eyebrow area\n",
    "    extended_eye_crop = 10\n",
    "    left_eye_x -= extended_eye_crop\n",
    "    left_eye_y -= extended_eye_crop\n",
    "    left_eye_w += 2 * extended_eye_crop\n",
    "    left_eye_h += 2 * extended_eye_crop\n",
    "\n",
    "    right_eye_x -= extended_eye_crop\n",
    "    right_eye_y -= extended_eye_crop\n",
    "    right_eye_w += 2 * extended_eye_crop\n",
    "    right_eye_h += 2 * extended_eye_crop\n",
    "\n",
    "    # Crop the eye regions from the frame.\n",
    "    left_eye_region = frame[left_eye_y:left_eye_y+left_eye_h, left_eye_x:left_eye_x+left_eye_w]\n",
    "    right_eye_region = frame[right_eye_y:right_eye_y+right_eye_h, right_eye_x:right_eye_x+right_eye_w]\n",
    "\n",
    "    # Convert the eye regions to grayscale.\n",
    "    left_eye_gray = cv2.cvtColor(left_eye_region, cv2.COLOR_BGR2GRAY)\n",
    "    right_eye_gray = cv2.cvtColor(right_eye_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return [left_eye_gray, right_eye_gray]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_image(image_path, img_size):\n",
    "    # Read the image in grayscale\n",
    "    img_array = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Resize the image while maintaining the aspect ratio\n",
    "    desired_size = (img_size, img_size)\n",
    "    height, width = img_array.shape\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    if aspect_ratio >= 1:\n",
    "        new_width = desired_size[0]\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = desired_size[1]\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    resized_image = cv2.resize(img_array, (new_width, new_height))\n",
    "\n",
    "    # Pad the resized image to make it square (img_size x img_size)\n",
    "    pad_width = (desired_size[1] - new_height) // 2\n",
    "    pad_height = (desired_size[0] - new_width) // 2\n",
    "    padded_image = np.pad(resized_image, ((pad_width, pad_width), (pad_height, pad_height)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Convert the grayscale image to RGB\n",
    "    rgb_image = cv2.cvtColor(padded_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[0.99973047]] right== [[0.65757334]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[0.9975064]] right== [[0.9964761]]\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[0.9950896]] right== [[0.99673194]]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[[0.9163406]] right== [[0.9941574]]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[0.29173943]] right== [[0.78045684]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[0.16062932]] right== [[0.8625472]]\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "[[0.30187184]] right== [[0.6227177]]\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.16848291]] right== [[0.5685371]]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "[[0.8579908]] right== [[0.91539794]]\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "[[0.2427327]] right== [[0.9261147]]\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[[0.20866324]] right== [[0.7769409]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[0.40525827]] right== [[0.01460869]]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "[[0.5270304]] right== [[0.01784305]]\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "[[0.9355103]] right== [[0.0609166]]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.98114085]] right== [[0.01409305]]\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[[0.00019546]] right== [[0.00747366]]\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[[0.28714105]] right== [[0.01202112]]\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[[0.36675408]] right== [[0.02345894]]\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.00226635]] right== [[0.00018852]]\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.2975468]] right== [[0.00112944]]\n",
      "ALERT: Eyes closed for several frames!\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[[4.3277254e-05]] right== [[0.00624029]]\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[0.00014074]] right== [[0.01110093]]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[[0.00104826]] right== [[0.01614163]]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[6.9257294e-05]] right== [[0.01495805]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.00029182]] right== [[0.01921097]]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[[5.8545076e-05]] right== [[0.03087303]]\n",
      "ALERT: Eyes closed for several frames!\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "[[0.01085911]] right== [[0.12534381]]\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "[[0.00575908]] right== [[0.02795827]]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[[0.00263104]] right== [[0.00474161]]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.9154772]] right== [[0.00691193]]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "[[0.9900341]] right== [[0.5461411]]\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.9990335]] right== [[0.6890051]]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[0.8372366]] right== [[0.06535725]]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[0.9999239]] right== [[0.15980811]]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[[0.24358341]] right== [[0.5449632]]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[0.96545947]] right== [[0.9877287]]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[[0.8696358]] right== [[0.98508537]]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[[0.99788195]] right== [[0.8443452]]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "[[0.21437337]] right== [[0.75946665]]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[0.999987]] right== [[0.9991032]]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "[[0.9993354]] right== [[0.99839216]]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[1.]] right== [[0.12905733]]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "[[1.]] right== [[0.00472293]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[[0.9996654]] right== [[0.14522645]]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[[1.369687e-06]] right== [[0.08854441]]\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "[[0.00028913]] right== [[0.99946785]]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[[3.7834514e-05]] right== [[0.00010684]]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[1.]] right== [[0.09620441]]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[[0.9999779]] right== [[0.5945806]]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.99995196]] right== [[0.76910615]]\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[[0.01170562]] right== [[2.2475397e-05]]\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "[[0.9728632]] right== [[0.17270638]]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[[0.7581982]] right== [[0.12964627]]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[0.99842453]] right== [[0.42092735]]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "[[0.00631324]] right== [[0.0006575]]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[0.9901123]] right== [[0.77741426]]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[[0.01718312]] right== [[0.8146705]]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[8.097391e-10]] right== [[0.99756974]]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[5.5835852e-12]] right== [[0.9973845]]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "[[0.9478073]] right== [[0.9992107]]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[[0.96870464]] right== [[0.94093376]]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(landmarks) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     38\u001b[0m     frame_without_face_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 39\u001b[0m     eyes \u001b[39m=\u001b[39m extract_eyes_from_landmarks(frame, landmarks[\u001b[39m0\u001b[39;49m])  \u001b[39m# Considering the first detected face\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     left_eye, right_eye \u001b[39m=\u001b[39m eyes[\u001b[39m0\u001b[39m], eyes[\u001b[39m1\u001b[39m]\n\u001b[0;32m     42\u001b[0m     \u001b[39m# Save the extracted eye images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 42\u001b[0m, in \u001b[0;36mextract_eyes_from_landmarks\u001b[1;34m(frame, landmarks)\u001b[0m\n\u001b[0;32m     39\u001b[0m right_eye_region \u001b[39m=\u001b[39m frame[right_eye_y:right_eye_y\u001b[39m+\u001b[39mright_eye_h, right_eye_x:right_eye_x\u001b[39m+\u001b[39mright_eye_w]\n\u001b[0;32m     41\u001b[0m \u001b[39m# Convert the eye regions to grayscale.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m left_eye_gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(left_eye_region, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[0;32m     43\u001b[0m right_eye_gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(right_eye_region, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m [left_eye_gray, right_eye_gray]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dlib face detector and facial landmark predictor models\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('..\\\\dlib_shape_predictor\\\\shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Load your drowsiness detection model\n",
    "#model = tf.keras.models.load_model('my_model.keras')\n",
    "model = tf.keras.models.load_model('my_model.keras', compile=False)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a directory to save the extracted eye images\n",
    "if not os.path.exists(\"eye_images\"):\n",
    "    os.makedirs(\"eye_images\")\n",
    "\n",
    "frame_without_face_count = 0\n",
    "frame_buffer = []\n",
    "closed_eye_frames_threshold = 5\n",
    "\n",
    "cv2.namedWindow('Drowsiness Detection')\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2.imshow('Drowsiness Detection', frame)\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Detect facial landmarks\n",
    "    landmarks = detect_face_landmarks(frame, face_detector, landmark_predictor)\n",
    "\n",
    "    # If a face is detected, extract eye regions from the frame\n",
    "    if len(landmarks) > 0:\n",
    "        frame_without_face_count = 0\n",
    "        eyes = extract_eyes_from_landmarks(frame, landmarks[0])  # Considering the first detected face\n",
    "        left_eye, right_eye = eyes[0], eyes[1]\n",
    "        \n",
    "        # Save the extracted eye images\n",
    "        cv2.imwrite(f\"eye_images/left_eye_{len(frame_buffer)}.jpg\", left_eye)\n",
    "        cv2.imwrite(f\"eye_images/right_eye_{len(frame_buffer)}.jpg\", right_eye)\n",
    "\n",
    "        left_eye = preprocess_single_image(f\"eye_images/left_eye_{len(frame_buffer)}.jpg\", img_size=100)\n",
    "        right_eye = preprocess_single_image(f\"eye_images/right_eye_{len(frame_buffer)}.jpg\", img_size=100)    \n",
    "        # Process the eye images with your drowsiness detection model\n",
    "        resized_image_l = cv2.resize(left_eye, (100, 100))\n",
    "        resized_image_r = cv2.resize(right_eye, (100, 100))\n",
    "        # Reshape the preprocessed image to match the input shape of the model\n",
    "        preprocessed_image_l = resized_image_l.reshape(-1, 100, 100, 3)\n",
    "        preprocessed_image_r = resized_image_r.reshape(-1, 100, 100, 3)\n",
    "\n",
    "        prediction_left = model.predict(preprocessed_image_l)  # Modify this line as per your model's input requirements\n",
    "        prediction_right = model.predict(preprocessed_image_r)\n",
    "        print(prediction_left, 'right==', prediction_right)\n",
    "        if (prediction_left < 0.8 and prediction_right < 0.8):  # Assuming 0.5 as the threshold for closed eyes\n",
    "            frame_buffer.append(True)\n",
    "        else:\n",
    "            frame_buffer.append(False)\n",
    "\n",
    "        if len(frame_buffer) > closed_eye_frames_threshold:\n",
    "            if all(frame_buffer[-closed_eye_frames_threshold:]):\n",
    "                print(\"ALERT: Eyes closed for several frames!\")\n",
    "                play_alert_sound()\n",
    "                # Trigger your alert mechanism here, e.g., sending an email, playing a sound, etc.\n",
    "                frame_buffer.clear()\n",
    "    else:\n",
    "        frame_without_face_count += 1\n",
    "        if frame_without_face_count >= 20:\n",
    "            print(\"ALERT: No face detected for several frames!\")\n",
    "            play_alert_sound()\n",
    "            # Trigger your alert mechanism here\n",
    "            frame_without_face_count = 0  # Reset the frame counter\n",
    "\n",
    "    # Exit the loop when 'e' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Load the dlib face detector and facial landmark predictor models\\nface_detector = dlib.get_frontal_face_detector()\\nlandmark_predictor = dlib.shape_predictor(\\'..\\\\dlib_shape_predictor\\\\shape_predictor_68_face_landmarks.dat\\')\\n\\n# Load your drowsiness detection model\\nmodel = tf.keras.models.load_model(\\'my_model.keras\\')\\n\\n# Open the webcam\\ncap = cv2.VideoCapture(0)\\n\\n# Create a directory to save the extracted eye images\\nif not os.path.exists(\"eye_images\"):\\n    os.makedirs(\"eye_images\")\\n\\nframe_buffer = []\\nclosed_eye_frames_threshold = 5\\nframe_without_face_count = 0  # Initialize a counter for frames without detecting a face\\nmax_frames_without_face = 20   # Define the maximum number of frames without a face\\n\\ncv2.namedWindow(\\'Drowsiness Detection\\')\\nwhile True:\\n    ret, frame = cap.read()\\n    frame = cv2.flip(frame, 1)\\n    cv2.imshow(\\'Drowsiness Detection\\', frame)\\n    if not ret:\\n        break\\n    \\n    # Detect facial landmarks\\n    landmarks = detect_face_landmarks(frame, face_detector, landmark_predictor)\\n\\n    # If a face is detected, extract eye regions from the frame\\n    if len(landmarks) > 0:\\n        frame_without_face_count = 0  # Reset the frame counter when a face is detected\\n        eyes = extract_eyes_from_landmarks(frame, landmarks[0])  # Considering the first detected face\\n        print(len(eyes))\\n        left_eye, right_eye = eyes[0], eyes[1]\\n        \\n        left_eye = preprocess_single_image(f\"eye_images/left_eye_{len(frame_buffer)}.jpg\", img_size=100)\\n        right_eye = preprocess_single_image(f\"eye_images/right_eye_{len(frame_buffer)}.jpg\", img_size=100)    \\n        # Process the eye ifmages with your drowsiness detection model\\n        resized_image_l = cv2.resize(left_eye, (100, 100))\\n        resized_image_r = cv2.resize(right_eye, (100, 100))\\n        # Reshape the preprocessed image to match the input shape of the model\\n        preprocessed_image_l = resized_image_l.reshape(-1, 100, 100, 3)\\n        preprocessed_image_r = resized_image_r.reshape(-1, 100, 100, 3)\\n\\n        prediction_left = model.predict(preprocessed_image_l)  # Modify this line as per your model\\'s input requirements\\n        prediction_right = model.predict(preprocessed_image_r)\\n        print(prediction_left,\\'right==\\',prediction_right)\\n        if (prediction_left < 0.5 and prediction_right < 0.5):  # Assuming 0.5 as the threshold for closed eyes\\n            frame_buffer.append(True)\\n        else:\\n            frame_buffer.append(False)\\n\\n\\n        if len(frame_buffer) > closed_eye_frames_threshold:\\n            if all(frame_buffer[-closed_eye_frames_threshold:]):\\n                print(\"ALERT: Eyes closed for several frames!\")\\n                play_alert_sound()\\n                # Trigger your alert mechanism here\\n                frame_buffer.clear()\\n    else:\\n        frame_without_face_count += 1\\n        if frame_without_face_count >= max_frames_without_face:\\n            print(\"ALERT: No face detected for several frames!\")\\n            play_alert_sound()\\n            # Trigger your alert mechanism here\\n            frame_without_face_count = 0  # Reset the frame counter\\n\\n    # Exit the loop when \\'e\\' key is pressed\\n    if cv2.waitKey(1) & 0xFF == ord(\\'e\\'):\\n        break\\n\\n# Release the webcam and close all windows\\ncap.release()\\ncv2.destroyAllWindows()'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Load the dlib face detector and facial landmark predictor models\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('..\\\\dlib_shape_predictor\\\\shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Load your drowsiness detection model\n",
    "model = tf.keras.models.load_model('my_model.keras')\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a directory to save the extracted eye images\n",
    "if not os.path.exists(\"eye_images\"):\n",
    "    os.makedirs(\"eye_images\")\n",
    "\n",
    "frame_buffer = []\n",
    "closed_eye_frames_threshold = 5\n",
    "frame_without_face_count = 0  # Initialize a counter for frames without detecting a face\n",
    "max_frames_without_face = 20   # Define the maximum number of frames without a face\n",
    "\n",
    "cv2.namedWindow('Drowsiness Detection')\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2.imshow('Drowsiness Detection', frame)\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Detect facial landmarks\n",
    "    landmarks = detect_face_landmarks(frame, face_detector, landmark_predictor)\n",
    "\n",
    "    # If a face is detected, extract eye regions from the frame\n",
    "    if len(landmarks) > 0:\n",
    "        frame_without_face_count = 0  # Reset the frame counter when a face is detected\n",
    "        eyes = extract_eyes_from_landmarks(frame, landmarks[0])  # Considering the first detected face\n",
    "        print(len(eyes))\n",
    "        left_eye, right_eye = eyes[0], eyes[1]\n",
    "        \n",
    "        left_eye = preprocess_single_image(f\"eye_images/left_eye_{len(frame_buffer)}.jpg\", img_size=100)\n",
    "        right_eye = preprocess_single_image(f\"eye_images/right_eye_{len(frame_buffer)}.jpg\", img_size=100)    \n",
    "        # Process the eye ifmages with your drowsiness detection model\n",
    "        resized_image_l = cv2.resize(left_eye, (100, 100))\n",
    "        resized_image_r = cv2.resize(right_eye, (100, 100))\n",
    "        # Reshape the preprocessed image to match the input shape of the model\n",
    "        preprocessed_image_l = resized_image_l.reshape(-1, 100, 100, 3)\n",
    "        preprocessed_image_r = resized_image_r.reshape(-1, 100, 100, 3)\n",
    "\n",
    "        prediction_left = model.predict(preprocessed_image_l)  # Modify this line as per your model's input requirements\n",
    "        prediction_right = model.predict(preprocessed_image_r)\n",
    "        print(prediction_left,'right==',prediction_right)\n",
    "        if (prediction_left < 0.5 and prediction_right < 0.5):  # Assuming 0.5 as the threshold for closed eyes\n",
    "            frame_buffer.append(True)\n",
    "        else:\n",
    "            frame_buffer.append(False)\n",
    "\n",
    "\n",
    "        if len(frame_buffer) > closed_eye_frames_threshold:\n",
    "            if all(frame_buffer[-closed_eye_frames_threshold:]):\n",
    "                print(\"ALERT: Eyes closed for several frames!\")\n",
    "                play_alert_sound()\n",
    "                # Trigger your alert mechanism here\n",
    "                frame_buffer.clear()\n",
    "    else:\n",
    "        frame_without_face_count += 1\n",
    "        if frame_without_face_count >= max_frames_without_face:\n",
    "            print(\"ALERT: No face detected for several frames!\")\n",
    "            play_alert_sound()\n",
    "            # Trigger your alert mechanism here\n",
    "            frame_without_face_count = 0  # Reset the frame counter\n",
    "\n",
    "    # Exit the loop when 'e' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
