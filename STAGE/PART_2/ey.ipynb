{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m global_image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdriver.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m global_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(global_image_path)\n\u001b[1;32m----> 3\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m'\u001b[39;49m\u001b[39mGlobal Image with Eye Detection\u001b[39;49m\u001b[39m'\u001b[39;49m, global_image)\n\u001b[0;32m      4\u001b[0m gray_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(global_image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "global_image_path = 'driver.jpg'\n",
    "global_image = cv2.imread(global_image_path)\n",
    "cv2.imshow('Global Image with Eye Detection', global_image)\n",
    "gray_image = cv2.cvtColor(global_image, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "eyes = eye_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_images = []\n",
    "for (x, y, w, h) in eyes:\n",
    "    eye_region = gray_image[y:y + h, x:x + w]\n",
    "    # Resize the extracted eye region to 64x64\n",
    "    eye_resized = cv2.resize(eye_region, (64, 64))\n",
    "    eye_images.append(eye_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m global_image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSTAGE/driver.jpg\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# Replace with the actual path to your global image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m global_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(global_image_path)\n\u001b[1;32m----> 6\u001b[0m gray_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(global_image, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[0;32m      8\u001b[0m eye_cascade \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mCascadeClassifier(cv2\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mhaarcascades \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhaarcascade_eye.xml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m eyes \u001b[39m=\u001b[39m eye_cascade\u001b[39m.\u001b[39mdetectMultiScale(gray_image, scaleFactor\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m, minNeighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, minSize\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m, \u001b[39m30\u001b[39m))\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global_image_path = 'STAGE/driver.jpg'  # Replace with the actual path to your global image\n",
    "global_image = cv2.imread(global_image_path)\n",
    "gray_image = cv2.cvtColor(global_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "eyes = eye_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "for (x, y, w, h) in eyes:\n",
    "    cv2.rectangle(global_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "# Convert BGR image to RGB for displaying with matplotlib\n",
    "global_image_rgb = cv2.cvtColor(global_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the global image with rectangles around the detected eyes using matplotlib\n",
    "plt.imshow(global_image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m global_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(global_image_path)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Convert the image to grayscale for eye detection\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m gray_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(global_image, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2GRAY)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Initialize the face detector and eye detector from dlib\u001b[39;00m\n\u001b[0;32m     13\u001b[0m face_detector \u001b[39m=\u001b[39m dlib\u001b[39m.\u001b[39mget_frontal_face_detector()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image using OpenCV\n",
    "global_image_path = 'driver.jpg'  # Replace with the actual path to your global image\n",
    "global_image = cv2.imread(global_image_path)\n",
    "\n",
    "# Convert the image to grayscale for eye detection\n",
    "gray_image = cv2.cvtColor(global_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initialize the face detector and eye detector from dlib\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\".\\\\dlib_shape_predictor\\\\shape_predictor_68_face_landmarks.dat\")\n",
    "# Detect faces in the image\n",
    "faces = face_detector(gray_image)\n",
    "\n",
    "# Lists to store the left and right eye images\n",
    "left_eye_images = []\n",
    "right_eye_images = []\n",
    "\n",
    "# Check if at least one face is detected\n",
    "if len(faces) > 0:\n",
    "    face = faces[0]  # Assuming there is only one face in the image\n",
    "\n",
    "    # Detect eyes for the first face detected\n",
    "    landmarks = eye_detector(gray_image, face)\n",
    "\n",
    "    # Get the coordinates of the left and right eyes in the landmarks\n",
    "    left_eye_x = landmarks.part(45).x\n",
    "    left_eye_y = landmarks.part(45).y\n",
    "    right_eye_x = landmarks.part(36).x\n",
    "    right_eye_y = landmarks.part(36).y\n",
    "\n",
    "    # Define the size of the cropped eye regions\n",
    "    eye_width = 64\n",
    "    eye_height = 48\n",
    "\n",
    "    # Crop and store the left and right eye images\n",
    "    left_eye_images.append(global_image[left_eye_y - eye_height // 2:left_eye_y + eye_height // 2,\n",
    "                                       left_eye_x - eye_width // 2:left_eye_x + eye_width // 2])\n",
    "\n",
    "    right_eye_images.append(global_image[right_eye_y - eye_height // 2:right_eye_y + eye_height // 2,\n",
    "                                        right_eye_x - eye_width // 2:right_eye_x + eye_width // 2])\n",
    "\n",
    "    # Display the left eye image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cv2.cvtColor(left_eye_images[0], cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title('Left Eye')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the right eye image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cv2.cvtColor(right_eye_images[0], cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title('Right Eye')\n",
    "    plt.show()\n",
    "\n",
    "# Note: Make sure to replace \"path_to_dlib_model.dat\" with the actual path to the pre-trained dlib model file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open dlib_shape_predictor\\shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m# Initialize the face detector and eye detector from dlib\u001b[39;00m\n\u001b[0;32m     28\u001b[0m face_detector \u001b[39m=\u001b[39m dlib\u001b[39m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m---> 29\u001b[0m predictor \u001b[39m=\u001b[39m dlib\u001b[39m.\u001b[39;49mshape_predictor(\u001b[39m\"\u001b[39;49m\u001b[39mdlib_shape_predictor\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mshape_predictor_68_face_landmarks.dat\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Detect faces in the image\u001b[39;00m\n\u001b[0;32m     32\u001b[0m faces \u001b[39m=\u001b[39m face_detector(gray_image)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open dlib_shape_predictor\\shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "def eye_aspect_ratio(eye_landmarks):\n",
    "    # Calculate the euclidean distances between the two sets of vertical eye landmarks\n",
    "    a = np.linalg.norm(np.array([eye_landmarks[1].x, eye_landmarks[1].y]) - np.array([eye_landmarks[5].x, eye_landmarks[5].y]))\n",
    "    b = np.linalg.norm(np.array([eye_landmarks[2].x, eye_landmarks[2].y]) - np.array([eye_landmarks[4].x, eye_landmarks[4].y]))\n",
    "\n",
    "    # Calculate the euclidean distance between the horizontal eye landmark\n",
    "    c = np.linalg.norm(np.array([eye_landmarks[0].x, eye_landmarks[0].y]) - np.array([eye_landmarks[3].x, eye_landmarks[3].y]))\n",
    "\n",
    "    # Calculate the EAR\n",
    "    ear = (a + b) / (2.0 * c)\n",
    "\n",
    "    return ear\n",
    "\n",
    "\n",
    "# Load the image using OpenCV\n",
    "global_image_path = 'driver2.jpg'  # Replace with the actual path to your global image\n",
    "global_image = cv2.imread(global_image_path)\n",
    "\n",
    "# Convert the image to grayscale for eye detection\n",
    "if global_image is not None:\n",
    "    gray_image = cv2.cvtColor(global_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initialize the face detector and eye detector from dlib\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"dlib_shape_predictor\\shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_detector(gray_image)\n",
    "\n",
    "# Lists to store the left and right eye images\n",
    "left_eye_images = []\n",
    "right_eye_images = []\n",
    "\n",
    "# Check if at least one face is detected\n",
    "if len(faces) > 0:\n",
    "    face = faces[0]  # Assuming there is only one face in the image\n",
    "\n",
    "    # Detect eyes for the first face detected\n",
    "    landmarks = predictor(gray_image, face)\n",
    "\n",
    "    # Get the coordinates of the left and right eyes in the landmarks\n",
    "    left_eye_x = landmarks.part(45).x\n",
    "    left_eye_y = landmarks.part(45).y\n",
    "    right_eye_x = landmarks.part(36).x\n",
    "    right_eye_y = landmarks.part(36).y\n",
    "\n",
    "    # Define the size of the cropped eye regions\n",
    "    eye_width = 64\n",
    "    eye_height = 48\n",
    "\n",
    "    # Crop and store the left and right eye images\n",
    "    left_eye_images.append(global_image[left_eye_y - eye_height // 2:left_eye_y + eye_height // 2,\n",
    "                                       left_eye_x - eye_width // 2:left_eye_x + eye_width // 2])\n",
    "\n",
    "    right_eye_images.append(global_image[right_eye_y - eye_height // 2:right_eye_y + eye_height // 2,\n",
    "                                        right_eye_x - eye_width // 2:right_eye_x + eye_width // 2])\n",
    "\n",
    "    # Calculate EAR for the left eye\n",
    "    left_eye_landmarks = landmarks.parts()[36:42]  # Landmarks 36 to 41 correspond to the right eye in dlib\n",
    "    left_eye_ear = eye_aspect_ratio(left_eye_landmarks)\n",
    "\n",
    "    # Calculate EAR for the right eye\n",
    "    right_eye_landmarks = landmarks.parts()[42:48]  # Landmarks 42 to 47 correspond to the left eye in dlib\n",
    "    right_eye_ear = eye_aspect_ratio(right_eye_landmarks)\n",
    "\n",
    "    print(f\"Left Eye EAR: {left_eye_ear}\")\n",
    "    print(f\"Right Eye EAR: {right_eye_ear}\")\n",
    "\n",
    "# Note: Make sure to replace \"path_to_dlib_model.dat\" with the actual path to the pre-trained dlib model file.\n",
    "    # Display the left eye image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cv2.cvtColor(left_eye_images[0], cv2.COLOR_BGR2RGB))\n",
    "    plt.savefig('opopop')\n",
    "    plt.axis('off')\n",
    "    plt.title(left_eye_ear)\n",
    "    plt.show()\n",
    "\n",
    "    # Display the right eye image\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(cv2.cvtColor(right_eye_images[0], cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(right_eye_ear)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
