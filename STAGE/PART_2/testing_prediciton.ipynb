{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_landmarks(frame, face_detector, landmark_predictor):\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector(gray_frame)\n",
    "    \n",
    "    landmarks_list = []\n",
    "    for face in faces:\n",
    "        landmarks = landmark_predictor(gray_frame, face)\n",
    "        landmarks_points = [(landmarks.part(n).x, landmarks.part(n).y) for n in range(68)]\n",
    "        landmarks_list.append(landmarks_points)\n",
    "    \n",
    "    return landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eyes_from_landmarks(frame, landmarks):\n",
    "    \n",
    "    if len(landmarks) != 68:\n",
    "        raise ValueError(\"Facial landmarks should contain 68 points.\")\n",
    "\n",
    "    # Define the indices for the left and right eyes based on facial landmarks.\n",
    "    left_eye_indices = [i for i in range(36, 42)]\n",
    "    right_eye_indices = [i for i in range(42, 48)]\n",
    "\n",
    "    # Extract left and right eye regions from the frame.\n",
    "    left_eye_coords = np.array([landmarks[i] for i in left_eye_indices], dtype=np.int32)\n",
    "    right_eye_coords = np.array([landmarks[i] for i in right_eye_indices], dtype=np.int32)\n",
    "\n",
    "    # Create a mask for each eye region.\n",
    "    mask_left = np.zeros_like(frame)\n",
    "    mask_right = np.zeros_like(frame)\n",
    "    cv2.fillPoly(mask_left, [left_eye_coords], (255, 255, 255))\n",
    "    cv2.fillPoly(mask_right, [right_eye_coords], (255, 255, 255))\n",
    "\n",
    "    # Apply the mask to the frame to get the eye regions.\n",
    "    left_eye_region = cv2.bitwise_and(frame, mask_left)\n",
    "    right_eye_region = cv2.bitwise_and(frame, mask_right)\n",
    "\n",
    "    # Convert the eye regions to grayscale for further processing if needed.\n",
    "    left_eye_gray = cv2.cvtColor(left_eye_region, cv2.COLOR_BGR2GRAY)\n",
    "    right_eye_gray = cv2.cvtColor(right_eye_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return [left_eye_gray, right_eye_gray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_image(image_path, img_size):\n",
    "    # Read the image in grayscale\n",
    "    img_array = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Resize the image while maintaining the aspect ratio\n",
    "    desired_size = (img_size, img_size)\n",
    "    height, width = img_array.shape\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    if aspect_ratio >= 1:\n",
    "        new_width = desired_size[0]\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = desired_size[1]\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "\n",
    "    resized_image = cv2.resize(img_array, (new_width, new_height))\n",
    "\n",
    "    # Pad the resized image to make it square (img_size x img_size)\n",
    "    pad_width = (desired_size[1] - new_height) // 2\n",
    "    pad_height = (desired_size[0] - new_width) // 2\n",
    "    padded_image = np.pad(resized_image, ((pad_width, pad_width), (pad_height, pad_height)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Convert the grayscale image to RGB\n",
    "    rgb_image = cv2.cvtColor(padded_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 100, 100, 3), found shape=(None, 100, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m right_eye \u001b[39m=\u001b[39m preprocess_single_image(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meye_images/right_eye_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(frame_buffer)\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m, img_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)    \n\u001b[0;32m     37\u001b[0m \u001b[39m# Process the eye images with your drowsiness detection model\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m prediction_left \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(left_eye)  \u001b[39m# Modify this line as per your model's input requirements\u001b[39;00m\n\u001b[0;32m     39\u001b[0m prediction_right \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(right_eye)\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m (prediction_left \u001b[39m<\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39mand\u001b[39;00m prediction_right \u001b[39m<\u001b[39m \u001b[39m0.5\u001b[39m):  \u001b[39m# Assuming 0.5 as the threshold for closed eyes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file7r0w0wu_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 100, 100, 3), found shape=(None, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the dlib face detector and facial landmark predictor models\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('..\\\\dlib_shape_predictor\\\\shape_predictor_68_face_landmarks.dat')\n",
    "# Load your drowsiness detection model\n",
    "model = tf.keras.models.load_model('my_model.keras')\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a directory to save the extracted eye images\n",
    "if not os.path.exists(\"eye_images\"):\n",
    "    os.makedirs(\"eye_images\")\n",
    "\n",
    "frame_buffer = []\n",
    "closed_eye_frames_threshold = 5\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Detect facial landmarks\n",
    "    landmarks = detect_face_landmarks(frame, face_detector, landmark_predictor)\n",
    "\n",
    "    # If a face is detected, extract eye regions from the frame\n",
    "    if len(landmarks) > 0:\n",
    "        eyes = extract_eyes_from_landmarks(frame, landmarks[0])  # Considering the first detected face\n",
    "        left_eye, right_eye = eyes[0], eyes[1]\n",
    "\n",
    "        # Save the extracted eye images\n",
    "        cv2.imwrite(f\"eye_images/left_eye_{len(frame_buffer)}.jpg\", left_eye)\n",
    "        cv2.imwrite(f\"eye_images/right_eye_{len(frame_buffer)}.jpg\", right_eye)\n",
    "\n",
    "        left_eye = preprocess_single_image(f\"eye_images/left_eye_{len(frame_buffer)}.jpg\", img_size=100)\n",
    "        right_eye = preprocess_single_image(f\"eye_images/right_eye_{len(frame_buffer)}.jpg\", img_size=100)    \n",
    "        # Process the eye images with your drowsiness detection model\n",
    "        prediction_left = model.predict(left_eye)  # Modify this line as per your model's input requirements\n",
    "        prediction_right = model.predict(right_eye)\n",
    "        if (prediction_left < 0.5 and prediction_right < 0.5):  # Assuming 0.5 as the threshold for closed eyes\n",
    "            frame_buffer.append(True)\n",
    "        else:\n",
    "            frame_buffer.append(False)\n",
    "\n",
    "        if len(frame_buffer) > closed_eye_frames_threshold:\n",
    "            if all(frame_buffer[-closed_eye_frames_threshold:]):\n",
    "                print(\"ALERT: Eyes closed for several frames!\")\n",
    "                # Trigger your alert mechanism here, e.g., sending an email, playing a sound, etc.\n",
    "                frame_buffer.clear()\n",
    "\n",
    "    # Exit the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (3,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m left_eye \u001b[39m=\u001b[39m preprocess_single_image(left_eye, img_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m right_eye \u001b[39m=\u001b[39m preprocess_single_image(right_eye, img_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPreprocessed left_eye shape:\u001b[39m\u001b[39m\"\u001b[39m, left_eye\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[25], line 26\u001b[0m, in \u001b[0;36mpreprocess_single_image\u001b[1;34m(image_array, img_size)\u001b[0m\n\u001b[0;32m     24\u001b[0m pad_width \u001b[39m=\u001b[39m (desired_size[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m new_height) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m     25\u001b[0m pad_height \u001b[39m=\u001b[39m (desired_size[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m new_width) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m---> 26\u001b[0m padded_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mpad(resized_image, ((pad_height, pad_height), (pad_width, pad_width)), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m'\u001b[39;49m, constant_values\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Convert the grayscale image to RGB\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rgb_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([padded_image] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraypad.py:744\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    741\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`pad_width` must be of integral type.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    743\u001b[0m \u001b[39m# Broadcast to shape (array.ndim, 2)\u001b[39;00m\n\u001b[1;32m--> 744\u001b[0m pad_width \u001b[39m=\u001b[39m _as_pairs(pad_width, array\u001b[39m.\u001b[39;49mndim, as_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(mode):\n\u001b[0;32m    747\u001b[0m     \u001b[39m# Old behavior: Use user-supplied function with np.apply_along_axis\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     function \u001b[39m=\u001b[39m mode\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraypad.py:518\u001b[0m, in \u001b[0;36m_as_pairs\u001b[1;34m(x, ndim, as_index)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mindex can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain negative values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m \u001b[39m# Converting the array with `tolist` seems to improve performance\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m# when iterating and indexing the result (see usage in `pad`)\u001b[39;00m\n\u001b[1;32m--> 518\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mbroadcast_to(x, (ndim, \u001b[39m2\u001b[39;49m))\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbroadcast_to\u001b[39m(array, shape, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    369\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[39m           [1, 2, 3]])\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m _broadcast_to(array, shape, subok\u001b[39m=\u001b[39;49msubok, readonly\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:349\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall elements of broadcast shape must be non-\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    347\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mnegative\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    348\u001b[0m extras \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 349\u001b[0m it \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mnditer(\n\u001b[0;32m    350\u001b[0m     (array,), flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mmulti_index\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrefs_ok\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mzerosize_ok\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m extras,\n\u001b[0;32m    351\u001b[0m     op_flags\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mreadonly\u001b[39;49m\u001b[39m'\u001b[39;49m], itershape\u001b[39m=\u001b[39;49mshape, order\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m it:\n\u001b[0;32m    353\u001b[0m     \u001b[39m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     broadcast \u001b[39m=\u001b[39m it\u001b[39m.\u001b[39mitviews[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (2,2)  and requested shape (3,2)"
     ]
    }
   ],
   "source": [
    "left_eye = preprocess_single_image(left_eye, img_size=100)\n",
    "right_eye = preprocess_single_image(right_eye, img_size=100)\n",
    "\n",
    "print(\"Preprocessed left_eye shape:\", left_eye.shape)\n",
    "print(\"Preprocessed right_eye shape:\", right_eye.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
